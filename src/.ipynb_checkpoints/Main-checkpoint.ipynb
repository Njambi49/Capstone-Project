{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249252ec-5f06-4157-a491-50af90393a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import download\n",
    "from gensim import corpora, models\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "download('stopwords')\n",
    "download('wordnet')\n",
    "\n",
    "def select_file():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    root.attributes('-topmost', True)  # Ensure root window is on top\n",
    "    file_paths = filedialog.askopenfilenames(parent=root)\n",
    "    if file_paths:\n",
    "        for file_path in file_paths:\n",
    "            process_file(file_path)\n",
    "\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path, sep='\\t', on_bad_lines='skip')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('english')) | {'br'}\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def process_file(file_path):\n",
    "    df = load_data(file_path)\n",
    "    df = df[['review_body']].dropna()\n",
    "    df['cleaned_review'] = df['review_body'].apply(preprocess_text)\n",
    "\n",
    "    text_data = [text.split() for text in df['cleaned_review']]\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "    num_topics = 5\n",
    "    ldamodel = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=1)\n",
    "    topics = ldamodel.print_topics(num_words=4)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "\n",
    "    # Dictionary to store word count by topic\n",
    "    topic_word_counts = {i: [] for i in range(num_topics)}\n",
    "\n",
    "    # Process each document to determine dominant topic and word count\n",
    "    for doc_id, doc in enumerate(corpus):\n",
    "        doc_topics = ldamodel.get_document_topics(doc, minimum_probability=0)\n",
    "        doc_topics = sorted(doc_topics, key=lambda x: -x[1])\n",
    "        dominant_topic, _ = doc_topics[0]\n",
    "        word_count = len(text_data[doc_id])\n",
    "        topic_word_counts[dominant_topic].append(word_count)\n",
    "\n",
    "    # Generate and display word clouds for each topic\n",
    "    for i in range(num_topics):\n",
    "        topic_words = dict(ldamodel.show_topic(i, 200))\n",
    "        wordcloud = WordCloud(width=400, height=400,\n",
    "                              background_color='white',\n",
    "                              stopwords=stopwords.words('english'),\n",
    "                              min_font_size=10).generate_from_frequencies(topic_words)\n",
    "\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Topic {i}\")\n",
    "        plt.show()\n",
    "\n",
    "    # Plot distribution of word counts by dominant topic\n",
    "    for i in range(num_topics):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(topic_word_counts[i], bins=30, kde=True, color='skyblue')\n",
    "        plt.title(f'Distribution of Document Word Counts for Topic {i}')\n",
    "        plt.xlabel('Word Count')\n",
    "        plt.ylabel('Number of Documents')\n",
    "        plt.show()\n",
    "\n",
    "    # Plot topic distribution across all documents\n",
    "    topic_distribution = [max(ldamodel[corpus[i]], key=lambda x: x[1])[0] for i in range(len(corpus))]\n",
    "    sns.histplot(topic_distribution, bins=num_topics, kde=False, discrete=True)\n",
    "    plt.title('Topic Distribution Across All Documents')\n",
    "    plt.xlabel('Topic')\n",
    "    plt.ylabel('Number of Documents')\n",
    "    plt.xticks(range(num_topics))\n",
    "    plt.show()\n",
    "\n",
    "    # Coherence scores for different numbers of topics\n",
    "    coherence_values = []\n",
    "    topic_range = range(2, 10)\n",
    "    for num in topic_range:\n",
    "        model = models.LdaModel(corpus, num_topics=num, id2word=dictionary, passes=1)\n",
    "        coherence_model = CoherenceModel(model=model, texts=text_data, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(topic_range, coherence_values, marker='o')\n",
    "    plt.title('Coherence Scores by Number of Topics')\n",
    "    plt.xlabel('Number of Topics')\n",
    "    plt.ylabel('Coherence Score')\n",
    "    plt.xticks(topic_range)\n",
    "    plt.show()\n",
    "\n",
    "    # Compute and visualize TF-IDF heatmap\n",
    "    cleaned_reviews = df['cleaned_review'].tolist()\n",
    "    vectorizer = TfidfVectorizer(max_features=50)\n",
    "    tfidf_matrix = vectorizer.fit_transform(cleaned_reviews).toarray()\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix, columns=terms)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(tfidf_df.T, cmap='YlGnBu', cbar=True)\n",
    "    plt.title('TF-IDF Scores for Top 50 Terms')\n",
    "    plt.xlabel('Document Index')\n",
    "    plt.ylabel('Term')\n",
    "    plt.show()\n",
    "\n",
    "    # Bar plot of top terms for each topic\n",
    "    for i in range(num_topics):\n",
    "        topic_words = ldamodel.show_topic(i, 10)\n",
    "        words = [word for word, _ in topic_words]\n",
    "        weights = [weight for _, weight in topic_words]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=weights, y=words, palette='viridis')\n",
    "        plt.title(f'Top Terms for Topic {i}')\n",
    "        plt.xlabel('Weight')\n",
    "        plt.ylabel('Term')\n",
    "        plt.show()\n",
    "\n",
    "# Call the select_file function to trigger the file selection dialog\n",
    "select_file()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
